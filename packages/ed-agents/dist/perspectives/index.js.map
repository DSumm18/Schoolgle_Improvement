{"version":3,"sources":["../../src/perspectives/index.ts","../../src/models/openrouter.ts","../../src/models/router.ts","../../src/perspectives/generator.ts"],"sourcesContent":["/**\r\n * Perspectives module exports\r\n */\r\n\r\nexport * from './generator';\r\n","/**\r\n * OpenRouter Client\r\n * Unified model access via OpenRouter API\r\n */\r\n\r\nimport type { ModelConfig, TokenUsage } from '../types';\r\n\r\n// ============================================================================\r\n// Configuration\r\n// ============================================================================\r\n\r\nexport interface OpenRouterConfig {\r\n  apiKey: string;\r\n  baseURL?: string;\r\n  timeout?: number;\r\n}\r\n\r\n// ============================================================================\r\n// Available Models on OpenRouter\r\n// ============================================================================\r\n\r\n/**\r\n * Model catalog with OpenRouter model IDs\r\n * Costs are approximate and may change - check openrouter.ai/docs for current pricing\r\n */\r\nexport const OPENROUTER_MODELS: Record<string, ModelConfig> = {\r\n  // ========== PREMIUM MODELS (High quality, higher cost) ==========\r\n\r\n  'anthropic/claude-3.5-sonnet': {\r\n    id: 'anthropic/claude-3.5-sonnet',\r\n    provider: 'openrouter',\r\n    model: 'anthropic/claude-3.5-sonnet',\r\n    costPerMillionTokens: 3.00, // Input, output is ~$15\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  'anthropic/claude-3.5-sonnet:beta': {\r\n    id: 'anthropic/claude-3.5-sonnet:beta',\r\n    provider: 'openrouter',\r\n    model: 'anthropic/claude-3.5-sonnet:beta',\r\n    costPerMillionTokens: 3.00,\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  'openai/gpt-4o': {\r\n    id: 'openai/gpt-4o',\r\n    provider: 'openrouter',\r\n    model: 'openai/gpt-4o',\r\n    costPerMillionTokens: 2.50, // Input, output is ~$10\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n\r\n  'google/gemini-2.0-flash-exp': {\r\n    id: 'google/gemini-2.0-flash-exp',\r\n    provider: 'openrouter',\r\n    model: 'google/gemini-2.0-flash-exp',\r\n    costPerMillionTokens: 0.075, // Very cheap!\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  'google/gemini-2.5-pro': {\r\n    id: 'google/gemini-2.5-pro',\r\n    provider: 'openrouter',\r\n    model: 'google/gemini-2.5-pro',\r\n    costPerMillionTokens: 1.25,\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  'google/gemini-2.5-flash-thinking-exp': {\r\n    id: 'google/gemini-2.5-flash-thinking-exp',\r\n    provider: 'openrouter',\r\n    model: 'google/gemini-2.5-flash-thinking-exp',\r\n    costPerMillionTokens: 0.10,\r\n    capabilities: {\r\n      vision: false,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  // ========== FAST CHEAP MODELS (Routing, classification) ==========\r\n\r\n  'openai/gpt-4o-mini': {\r\n    id: 'openai/gpt-4o-mini',\r\n    provider: 'openrouter',\r\n    model: 'openai/gpt-4o-mini',\r\n    costPerMillionTokens: 0.15, // Input, output is ~$0.60\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n\r\n  'deepseek/deepseek-chat': {\r\n    id: 'deepseek/deepseek-chat',\r\n    provider: 'openrouter',\r\n    model: 'deepseek/deepseek-chat',\r\n    costPerMillionTokens: 0.27, // Input, output is ~$1.10\r\n    capabilities: {\r\n      vision: false,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n\r\n  'deepseek/deepseek-chat-v3-0324': {\r\n    id: 'deepseek/deepseek-chat-v3-0324',\r\n    provider: 'openrouter',\r\n    model: 'deepseek/deepseek-chat-v3-0324',\r\n    costPerMillionTokens: 0.27,\r\n    capabilities: {\r\n      vision: false,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n\r\n  'deepseek/deepseek-r1': {\r\n    id: 'deepseek/deepseek-r1',\r\n    provider: 'openrouter',\r\n    model: 'deepseek/deepseek-r1',\r\n    costPerMillionTokens: 0.55, // Reasoning model\r\n    capabilities: {\r\n      vision: false,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n\r\n  // ========== ULTRA-CHEAP MODELS ==========\r\n\r\n  // Note: Free models change frequently - check openrouter.ai for current free options\r\n\r\n  // ========== VISION MODELS ==========\r\n\r\n  'anthropic/claude-3.5-sonnet': {\r\n    id: 'anthropic/claude-3.5-sonnet',\r\n    provider: 'openrouter',\r\n    model: 'anthropic/claude-3.5-sonnet',\r\n    costPerMillionTokens: 3.00,\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  'openai/gpt-4o': {\r\n    id: 'openai/gpt-4o',\r\n    provider: 'openrouter',\r\n    model: 'openai/gpt-4o',\r\n    costPerMillionTokens: 2.50,\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n\r\n  'google/gemini-2.0-flash-exp': {\r\n    id: 'google/gemini-2.0-flash-exp',\r\n    provider: 'openrouter',\r\n    model: 'google/gemini-2.0-flash-exp',\r\n    costPerMillionTokens: 0.075,\r\n    capabilities: {\r\n      vision: true,\r\n      streaming: true,\r\n      jsonMode: false,\r\n    },\r\n  },\r\n\r\n  // ========== REASONING MODELS ==========\r\n\r\n  'deepseek/deepseek-r1': {\r\n    id: 'deepseek/deepseek-r1',\r\n    provider: 'openrouter',\r\n    model: 'deepseek/deepseek-r1',\r\n    costPerMillionTokens: 0.55,\r\n    capabilities: {\r\n      vision: false,\r\n      streaming: true,\r\n      jsonMode: true,\r\n    },\r\n  },\r\n};\r\n\r\n/**\r\n * Model aliases for easy reference\r\n */\r\nexport const MODEL_ALIASES: Record<string, string> = {\r\n  // Primary models\r\n  'premium': 'anthropic/claude-3.5-sonnet',\r\n  'fast': 'openai/gpt-4o-mini',\r\n  'cheap': 'deepseek/deepseek-chat',\r\n\r\n  // Specific model aliases\r\n  'claude': 'anthropic/claude-3.5-sonnet',\r\n  'gpt4': 'openai/gpt-4o',\r\n  'gpt4-mini': 'openai/gpt-4o-mini',\r\n  'gemini': 'google/gemini-2.5-flash-thinking-exp',\r\n  'deepseek': 'deepseek/deepseek-chat',\r\n  'deepseek-r1': 'deepseek/deepseek-r1',\r\n};\r\n\r\n// ============================================================================\r\n// OpenRouter Client Class\r\n// ============================================================================\r\n\r\nexport interface ChatMessage {\r\n  role: 'system' | 'user' | 'assistant';\r\n  content: string;\r\n}\r\n\r\nexport interface ChatOptions {\r\n  model?: string;\r\n  temperature?: number;\r\n  maxTokens?: number;\r\n  topP?: number;\r\n  stream?: boolean;\r\n}\r\n\r\nexport interface ChatResponse {\r\n  content: string;\r\n  model: string;\r\n  usage: {\r\n    promptTokens: number;\r\n    completionTokens: number;\r\n    totalTokens: number;\r\n  };\r\n  finishReason?: string;\r\n}\r\n\r\n/**\r\n * OpenRouter API Client\r\n */\r\nexport class OpenRouterClient {\r\n  private config: OpenRouterConfig;\r\n  private baseURL: string;\r\n\r\n  constructor(config: OpenRouterConfig) {\r\n    this.config = config;\r\n    this.baseURL = config.baseURL || 'https://openrouter.ai/api/v1';\r\n  }\r\n\r\n  /**\r\n   * Send a chat completion request\r\n   */\r\n  async chat(\r\n    messages: ChatMessage[],\r\n    options: ChatOptions = {}\r\n  ): Promise<ChatResponse> {\r\n    const model = options.model || 'anthropic/claude-3.5-sonnet';\r\n\r\n    const requestBody = {\r\n      model,\r\n      messages: messages.map(m => ({\r\n        role: m.role,\r\n        content: m.content,\r\n      })),\r\n      temperature: options.temperature ?? 0.7,\r\n      max_tokens: options.maxTokens ?? 4096,\r\n      top_p: options.topP,\r\n      stream: options.stream ?? false,\r\n    };\r\n\r\n    const response = await fetch(`${this.baseURL}/chat/completions`, {\r\n      method: 'POST',\r\n      headers: {\r\n        'Authorization': `Bearer ${this.config.apiKey}`,\r\n        'Content-Type': 'application/json',\r\n        'HTTP-Referer': typeof window !== 'undefined' ? window.location.href : 'https://schoolgle.co.uk',\r\n        'X-Title': 'Schoolgle Ed',\r\n      },\r\n      body: JSON.stringify(requestBody),\r\n      signal: options.timeout ? AbortSignal.timeout(options.timeout) : undefined,\r\n    });\r\n\r\n    if (!response.ok) {\r\n      const errorText = await response.text();\r\n      throw new OpenRouterError(\r\n        `OpenRouter API error: ${response.status} ${response.statusText}`,\r\n        response.status,\r\n        errorText\r\n      );\r\n    }\r\n\r\n    const data = await response.json();\r\n\r\n    // Extract response\r\n    const choice = data.choices?.[0];\r\n    if (!choice) {\r\n      throw new OpenRouterError('No choices returned from OpenRouter', 500);\r\n    }\r\n\r\n    return {\r\n      content: choice.message?.content || '',\r\n      model: data.model || model,\r\n      usage: {\r\n        promptTokens: data.usage?.prompt_tokens || 0,\r\n        completionTokens: data.usage?.completion_tokens || 0,\r\n        totalTokens: data.usage?.total_tokens || 0,\r\n      },\r\n      finishReason: choice.finish_reason,\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Send a chat completion request with system prompt\r\n   */\r\n  async chatWithSystem(\r\n    systemPrompt: string,\r\n    userMessage: string,\r\n    options: ChatOptions = {}\r\n  ): Promise<ChatResponse> {\r\n    return this.chat(\r\n      [\r\n        { role: 'system', content: systemPrompt },\r\n        { role: 'user', content: userMessage },\r\n      ],\r\n      options\r\n    );\r\n  }\r\n\r\n  /**\r\n   * Stream a chat completion (for future implementation)\r\n   */\r\n  async *chatStream(\r\n    messages: ChatMessage[],\r\n    options: ChatOptions = {}\r\n  ): AsyncGenerator<string, void, unknown> {\r\n    const model = options.model || 'anthropic/claude-3.5-sonnet';\r\n\r\n    const requestBody = {\r\n      model,\r\n      messages: messages.map(m => ({\r\n        role: m.role,\r\n        content: m.content,\r\n      })),\r\n      temperature: options.temperature ?? 0.7,\r\n      max_tokens: options.maxTokens ?? 4096,\r\n      stream: true,\r\n    };\r\n\r\n    const response = await fetch(`${this.baseURL}/chat/completions`, {\r\n      method: 'POST',\r\n      headers: {\r\n        'Authorization': `Bearer ${this.config.apiKey}`,\r\n        'Content-Type': 'application/json',\r\n        'HTTP-Referer': 'https://schoolgle.co.uk',\r\n        'X-Title': 'Schoolgle Ed',\r\n      },\r\n      body: JSON.stringify(requestBody),\r\n    });\r\n\r\n    if (!response.ok) {\r\n      throw new OpenRouterError(\r\n        `OpenRouter API error: ${response.status} ${response.statusText}`,\r\n        response.status\r\n      );\r\n    }\r\n\r\n    // Stream parsing\r\n    const reader = response.body?.getReader();\r\n    if (!reader) throw new OpenRouterError('No response body', 500);\r\n\r\n    const decoder = new TextDecoder();\r\n\r\n    try {\r\n      while (true) {\r\n        const { done, value } = await reader.read();\r\n        if (done) break;\r\n\r\n        const chunk = decoder.decode(value);\r\n        const lines = chunk.split('\\n').filter(line => line.trim());\r\n\r\n        for (const line of lines) {\r\n          if (line.startsWith('data: ')) {\r\n            const data = line.slice(6);\r\n            if (data === '[DONE]') return;\r\n\r\n            try {\r\n              const parsed = JSON.parse(data);\r\n              const content = parsed.choices?.[0]?.delta?.content;\r\n              if (content) yield content;\r\n            } catch {\r\n              // Skip invalid JSON\r\n            }\r\n          }\r\n        }\r\n      }\r\n    } finally {\r\n      reader.releaseLock();\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Get model info\r\n   */\r\n  getModel(modelIdOrAlias: string): ModelConfig | undefined {\r\n    // Resolve alias\r\n    const modelId = MODEL_ALIASES[modelIdOrAlias] || modelIdOrAlias;\r\n    return OPENROUTER_MODELS[modelId];\r\n  }\r\n\r\n  /**\r\n   * Get all available models\r\n   */\r\n  getAllModels(): Record<string, ModelConfig> {\r\n    return { ...OPENROUTER_MODELS };\r\n  }\r\n}\r\n\r\n// ============================================================================\r\n// Error Class\r\n// ============================================================================\r\n\r\nexport class OpenRouterError extends Error {\r\n  constructor(\r\n    message: string,\r\n    public statusCode: number,\r\n    public responseText?: string\r\n  ) {\r\n    super(message);\r\n    this.name = 'OpenRouterError';\r\n  }\r\n}\r\n\r\n// ============================================================================\r\n// Utility Functions\r\n// ============================================================================\r\n\r\n/**\r\n * Create OpenRouter client from environment\r\n */\r\nexport function createOpenRouterClient(): OpenRouterClient {\r\n  const apiKey = process.env.OPENROUTER_API_KEY || '';\r\n\r\n  if (!apiKey) {\r\n    throw new Error('OPENROUTER_API_KEY environment variable is not set');\r\n  }\r\n\r\n  return new OpenRouterClient({\r\n    apiKey,\r\n    timeout: 30000, // 30 second default\r\n  });\r\n}\r\n\r\n/**\r\n * Calculate token cost\r\n */\r\nexport function calculateTokenCost(\r\n  modelId: string,\r\n  inputTokens: number,\r\n  outputTokens: number\r\n): number {\r\n  const model = OPENROUTER_MODELS[modelId];\r\n  if (!model) return 0;\r\n\r\n  // Note: OpenRouter pricing can vary by provider\r\n  // This is an approximation using the base cost per million tokens\r\n  const inputCost = (inputTokens / 1_000_000) * model.costPerMillionTokens;\r\n  const outputCost = (outputTokens / 1_000_000) * (model.costPerMillionTokens * 3); // Rough estimate\r\n\r\n  return inputCost + outputCost;\r\n}\r\n","/**\r\n * Model Router\r\n * Selects the best model for each task using OpenRouter\r\n */\r\n\r\nimport type { ModelConfig, TaskType, AppContext } from '../types';\r\nimport {\r\n  OPENROUTER_MODELS,\r\n  MODEL_ALIASES,\r\n  createOpenRouterClient,\r\n  calculateTokenCost as calculateORCost,\r\n  type ChatMessage,\r\n  type ChatOptions,\r\n  type ChatResponse,\r\n} from './openrouter';\r\n\r\n// ============================================================================\r\n// Task-to-Model Mapping\r\n// ============================================================================\r\n\r\n/**\r\n * Default model selection by task type\r\n * Ordered by preference (first is optimal, last is fallback)\r\n */\r\nconst TASK_MODEL_MAP: Record<TaskType, string[]> = {\r\n  // Fast/cheap for routing\r\n  'intent-classification': ['openai/gpt-4o-mini', 'google/gemini-2.0-flash-exp', 'deepseek/deepseek-chat'],\r\n  'work-focus-check': ['openai/gpt-4o-mini', 'google/gemini-2.0-flash-exp'],\r\n\r\n  // High quality for specialist responses\r\n  'specialist-response': ['anthropic/claude-3.5-sonnet', 'deepseek/deepseek-chat', 'openai/gpt-4o'],\r\n  'perspective-generation': ['deepseek/deepseek-chat', 'openai/gpt-4o-mini', 'google/gemini-2.0-flash-exp'],\r\n  'synthesis': ['anthropic/claude-3.5-sonnet', 'deepseek/deepseek-chat'],\r\n\r\n  // Vision needed\r\n  'ui-analysis': ['anthropic/claude-3.5-sonnet', 'openai/gpt-4o', 'google/gemini-2.5-pro'],\r\n\r\n  // Fast/cheap for actions\r\n  'action-planning': ['openai/gpt-4o-mini', 'google/gemini-2.0-flash-exp'],\r\n};\r\n\r\n/**\r\n * Plan-based model constraints\r\n */\r\nconst PLAN_MODEL_CONSTRAINTS: Record<string, string[]> = {\r\n  'free': ['openai/gpt-4o-mini', 'google/gemini-2.0-flash-exp', 'deepseek/deepseek-chat'],\r\n  'schools': ['anthropic/claude-3.5-sonnet', 'deepseek/deepseek-chat', 'openai/gpt-4o', 'google/gemini-2.0-flash-exp'],\r\n  'trusts': ['anthropic/claude-3.5-sonnet', 'deepseek/deepseek-r1', 'openai/gpt-4o'],\r\n};\r\n\r\n// ============================================================================\r\n// Model Router\r\n// ============================================================================\r\n\r\n/**\r\n * Model Router class\r\n */\r\nexport class ModelRouter {\r\n  private client: ReturnType<typeof createOpenRouterClient>;\r\n\r\n  constructor() {\r\n    this.client = createOpenRouterClient();\r\n  }\r\n\r\n  /**\r\n   * Select the best model for a given task based on context\r\n   */\r\n  selectModel(task: TaskType, context: AppContext): ModelConfig {\r\n    const availableModels = TASK_MODEL_MAP[task] || TASK_MODEL_MAP['specialist-response'];\r\n    const { plan, creditsRemaining } = context.subscription;\r\n\r\n    // Filter models by plan\r\n    const planModels = PLAN_MODEL_CONSTRAINTS[plan] || availableModels;\r\n    const eligibleModels = availableModels.filter(m => planModels.includes(m));\r\n\r\n    // If low credits, use cheapest option\r\n    if (creditsRemaining < 1000) {\r\n      return this.getCheapestModel(eligibleModels);\r\n    }\r\n\r\n    // Otherwise use optimal (first) model\r\n    const modelId = eligibleModels[0];\r\n    const model = OPENROUTER_MODELS[modelId];\r\n\r\n    if (!model) {\r\n      throw new Error(`Model not found: ${modelId}`);\r\n    }\r\n\r\n    return model;\r\n  }\r\n\r\n  /**\r\n   * Send chat completion request\r\n   */\r\n  async chat(\r\n    systemPrompt: string,\r\n    userMessage: string,\r\n    options: ChatOptions = {}\r\n  ): Promise<ChatResponse> {\r\n    return this.client.chatWithSystem(systemPrompt, userMessage, options);\r\n  }\r\n\r\n  /**\r\n   * Send chat completion with message array\r\n   */\r\n  async chatMessages(\r\n    messages: ChatMessage[],\r\n    options: ChatOptions = {}\r\n  ): Promise<ChatResponse> {\r\n    return this.client.chat(messages, options);\r\n  }\r\n\r\n  /**\r\n   * Stream chat completion (for future use)\r\n   */\r\n  async *chatStream(\r\n    messages: ChatMessage[],\r\n    options: ChatOptions = {}\r\n  ): AsyncGenerator<string> {\r\n    yield* this.client.chatStream(messages, options);\r\n  }\r\n\r\n  /**\r\n   * Get cheapest model from list\r\n   */\r\n  private getCheapestModel(modelIds: string[]): ModelConfig {\r\n    let cheapest = OPENROUTER_MODELS[modelIds[0]];\r\n    let lowestCost = cheapest?.costPerMillionTokens || Infinity;\r\n\r\n    for (const modelId of modelIds) {\r\n      const model = OPENROUTER_MODELS[modelId];\r\n      if (model && model.costPerMillionTokens < lowestCost) {\r\n        cheapest = model;\r\n        lowestCost = model.costPerMillionTokens;\r\n      }\r\n    }\r\n\r\n    return cheapest || OPENROUTER_MODELS['openai/gpt-4o-mini'];\r\n  }\r\n\r\n  /**\r\n   * Get model by ID or alias\r\n   */\r\n  getModel(idOrAlias: string): ModelConfig | undefined {\r\n    const modelId = MODEL_ALIASES[idOrAlias] || idOrAlias;\r\n    return OPENROUTER_MODELS[modelId];\r\n  }\r\n\r\n  /**\r\n   * Get all available models\r\n   */\r\n  getAllModels(): Record<string, ModelConfig> {\r\n    return { ...OPENROUTER_MODELS };\r\n  }\r\n}\r\n\r\n// Singleton instance\r\nlet routerInstance: ModelRouter | null = null;\r\n\r\n/**\r\n * Get or create model router instance\r\n */\r\nexport function getModelRouter(): ModelRouter {\r\n  if (!routerInstance) {\r\n    routerInstance = new ModelRouter();\r\n  }\r\n  return routerInstance;\r\n}\r\n\r\n// ============================================================================\r\n// Legacy Compatibility Functions\r\n// ============================================================================\r\n\r\n/**\r\n * Select model for task (legacy function signature)\r\n */\r\nexport async function selectModel(\r\n  task: TaskType,\r\n  context: AppContext\r\n): Promise<ModelConfig> {\r\n  const router = getModelRouter();\r\n  return router.selectModel(task, context);\r\n}\r\n\r\n/**\r\n * Check credits (simplified)\r\n */\r\nexport async function checkCredits(subscription: { creditsRemaining: number }): Promise<{\r\n  sufficient: boolean;\r\n  estimatedCost: number;\r\n}> {\r\n  // Estimate for typical query\r\n  const estimatedTokens = 1000;\r\n  const model = OPENROUTER_MODELS['google/gemini-2.0-flash-exp'];\r\n  const estimatedCost = (estimatedTokens / 1_000_000) * model.costPerMillionTokens;\r\n\r\n  return {\r\n    sufficient: subscription.creditsRemaining > estimatedCost,\r\n    estimatedCost,\r\n  };\r\n}\r\n\r\n/**\r\n * Calculate cost\r\n */\r\nexport function calculateCost(\r\n  modelId: string,\r\n  inputTokens: number,\r\n  outputTokens: number\r\n): number {\r\n  return calculateORCost(modelId, inputTokens, outputTokens);\r\n}\r\n\r\n/**\r\n * Get model by ID\r\n */\r\nexport function getModel(id: string): ModelConfig | undefined {\r\n  const router = getModelRouter();\r\n  return router.getModel(id);\r\n}\r\n\r\n/**\r\n * Get all models\r\n */\r\nexport function getAllModels(): Record<string, ModelConfig> {\r\n  const router = getModelRouter();\r\n  return router.getAllModels();\r\n}\r\n","/**\r\n * Multi-Perspective Generator\r\n * Generates optimist, critic, and neutral perspectives for balanced responses\r\n */\r\n\r\nimport type { PerspectiveType, PerspectiveResponse, AppContext } from '../types';\r\nimport { getModelRouter } from '../models';\r\n\r\n// ============================================================================\r\n// Perspective Prompts\r\n// ============================================================================\r\n\r\n/**\r\n * Optimist perspective prompt\r\n */\r\nexport const OPTIMIST_PROMPT = `You are the OPTIMIST perspective.\r\n\r\nYour role is to highlight:\r\n- What's working well\r\n- What's possible\r\n- Positive outcomes\r\n- Encouraging aspects\r\n- Benefits and opportunities\r\n\r\nKeep it brief (2-3 sentences max). Focus on possibilities and what could go right.\r\n\r\n**Important:** Be specific to the question asked, not generic positivity.`;\r\n\r\n/**\r\n * Critic perspective prompt\r\n */\r\nexport const CRITIC_PROMPT = `You are the CRITIC perspective (devil's advocate).\r\n\r\nYour role is to highlight:\r\n- What could go wrong\r\n- Risks and concerns\r\n- Missing information\r\n- Areas that need caution\r\n- Potential pitfalls\r\n\r\nKeep it brief (2-3 sentences max). Focus on safety and what could go wrong.\r\n\r\n**Important:** Be specific to the question asked, not generic negativity.`;\r\n\r\n/**\r\n * Neutral perspective prompt\r\n */\r\nexport const NEUTRAL_PROMPT = `You are the NEUTRAL perspective.\r\n\r\nYour role is to provide:\r\n- Balanced factual summary\r\n- Key points only\r\n- No bias either way\r\n- Objective assessment\r\n\r\nKeep it brief (2-3 sentences max). Focus on facts.\r\n\r\n**Important:** Be specific to the question asked, avoid vague statements.`;\r\n\r\n/**\r\n * Synthesis prompt\r\n */\r\nexport const SYNTHESIS_PROMPT = `You are a SYNTHESIZER.\r\n\r\nYour role is to combine three perspectives into a balanced, actionable response.\r\n\r\nYou will receive:\r\n1. The original question\r\n2. An expert's answer\r\n3. Three perspectives: optimist, critic, and neutral\r\n\r\nCreate a response that:\r\n- Acknowledges the expert guidance\r\n- Incorporates valid points from all perspectives\r\n- Provides a balanced view of the situation\r\n- Ends with clear, actionable next steps\r\n\r\nBe concise but comprehensive. Use markdown formatting for readability.`;\r\n\r\n// ============================================================================\r\n// Perspective Generation\r\n// ============================================================================\r\n\r\n/**\r\n * Generate multi-perspective response\r\n */\r\nexport async function generateMultiPerspectiveResponse(\r\n  question: string,\r\n  specialistResponse: string,\r\n  context: AppContext\r\n): Promise<{\r\n  synthesized: string;\r\n  perspectives: {\r\n    optimist?: string;\r\n    critic?: string;\r\n    neutral?: string;\r\n  };\r\n}> {\r\n  // Use cheaper model for perspectives (they're shorter, less critical)\r\n  const perspectiveModelId = 'deepseek/deepseek-chat';\r\n  const synthesisModelId = 'anthropic/claude-3.5-sonnet';\r\n\r\n  try {\r\n    // Generate three perspectives in parallel\r\n    const [optimist, critic, neutral] = await Promise.all([\r\n      generatePerspective(question, specialistResponse, 'optimist', context, perspectiveModelId),\r\n      generatePerspective(question, specialistResponse, 'critic', context, perspectiveModelId),\r\n      generatePerspective(question, specialistResponse, 'neutral', context, perspectiveModelId),\r\n    ]);\r\n\r\n    // Synthesize into balanced response\r\n    const synthesized = await synthesizeResponse(question, specialistResponse, {\r\n      optimist,\r\n      critic,\r\n      neutral,\r\n    }, context, synthesisModelId);\r\n\r\n    return {\r\n      synthesized,\r\n      perspectives: {\r\n        optimist,\r\n        critic,\r\n        neutral,\r\n      },\r\n    };\r\n  } catch (error) {\r\n    // If perspective generation fails, return specialist response with warning\r\n    console.error('Perspective generation failed:', error);\r\n    return {\r\n      synthesized: `${specialistResponse}\\n\\n*Note: Unable to generate additional perspectives at this time.*`,\r\n      perspectives: undefined,\r\n    };\r\n  }\r\n}\r\n\r\n/**\r\n * Generate a single perspective\r\n */\r\nasync function generatePerspective(\r\n  question: string,\r\n  specialistResponse: string,\r\n  type: PerspectiveType,\r\n  context: AppContext,\r\n  modelId: string\r\n): Promise<string> {\r\n  const router = getModelRouter();\r\n  const prompt = getPerspectivePrompt(type);\r\n\r\n  const userMessage = `\r\n**Question:** ${question}\r\n\r\n**Specialist Response:** ${specialistResponse}\r\n\r\nProvide your ${type} perspective on this guidance.`;\r\n\r\n  try {\r\n    const response = await router.chat(\r\n      prompt,\r\n      userMessage,\r\n      {\r\n        model: modelId,\r\n        temperature: 0.7,\r\n        maxTokens: 200, // Perspectives should be brief\r\n      }\r\n    );\r\n\r\n    return response.content.trim();\r\n  } catch (error) {\r\n    // Return fallback if generation fails\r\n    return getFallbackPerspective(type, question);\r\n  }\r\n}\r\n\r\n/**\r\n * Get perspective prompt by type\r\n */\r\nfunction getPerspectivePrompt(type: PerspectiveType): string {\r\n  switch (type) {\r\n    case 'optimist':\r\n      return OPTIMIST_PROMPT;\r\n    case 'critic':\r\n      return CRITIC_PROMPT;\r\n    case 'neutral':\r\n      return NEUTRAL_PROMPT;\r\n  }\r\n}\r\n\r\n/**\r\n * Synthesize multiple perspectives into final response\r\n */\r\nasync function synthesizeResponse(\r\n  question: string,\r\n  specialistResponse: string,\r\n  perspectives: {\r\n    optimist: string;\r\n    critic: string;\r\n    neutral: string;\r\n  },\r\n  context: AppContext,\r\n  modelId: string\r\n): Promise<string> {\r\n  const router = getModelRouter();\r\n\r\n  const userMessage = `\r\n**Question:** ${question}\r\n\r\n**Specialist Response:**\r\n${specialistResponse}\r\n\r\n**Perspectives:**\r\n\r\n**Optimist says:**\r\n${perspectives.optimist}\r\n\r\n**Critic says:**\r\n${perspectives.critic}\r\n\r\n**Neutral says:**\r\n${perspectives.neutral}\r\n\r\nPlease synthesize this into a balanced, actionable response.`;\r\n\r\n  try {\r\n    const response = await router.chat(\r\n      SYNTHESIS_PROMPT,\r\n      userMessage,\r\n      {\r\n        model: modelId,\r\n        temperature: 0.7,\r\n        maxTokens: 1000,\r\n      }\r\n    );\r\n\r\n    return response.content.trim();\r\n  } catch (error) {\r\n    // Fallback to simple format if synthesis fails\r\n    return formatFallbackSynthesis(specialistResponse, perspectives);\r\n  }\r\n}\r\n\r\n/**\r\n * Get fallback perspective when generation fails\r\n */\r\nfunction getFallbackPerspective(type: PerspectiveType, question: string): string {\r\n  switch (type) {\r\n    case 'optimist':\r\n      return \"From a positive perspective, this is a solvable challenge with clear steps forward. Following the guidance should lead to successful outcomes.\";\r\n    case 'critic':\r\n      return \"From a cautious perspective, ensure you verify all guidance is current and follow procedures carefully. Don't cut corners on safety or compliance.\";\r\n    case 'neutral':\r\n      return \"From a balanced perspective, follow the established procedures and verify guidance for your specific situation.\";\r\n  }\r\n}\r\n\r\n/**\r\n * Format fallback synthesis when LLM fails\r\n */\r\nfunction formatFallbackSynthesis(\r\n  specialistResponse: string,\r\n  perspectives: {\r\n    optimist: string;\r\n    critic: string;\r\n    neutral: string;\r\n  }\r\n): string {\r\n  return `${specialistResponse}\r\n\r\n---\r\n\r\n### Additional Perspectives\r\n\r\n**Optimistic View:**\r\n${perspectives.optimist}\r\n\r\n**Cautious View:**\r\n${perspectives.critic}\r\n\r\n**Balanced View:**\r\n${perspectives.neutral}`;\r\n}\r\n\r\n// ============================================================================\r\n// Perspective Caching\r\n// ============================================================================\r\n\r\n/**\r\n * Simple in-memory cache for perspectives (to reduce API calls)\r\n */\r\nconst perspectiveCache = new Map<string, {\r\n  perspectives: PerspectiveResponse;\r\n  timestamp: number;\r\n}>();\r\n\r\nconst CACHE_TTL = 1000 * 60 * 60; // 1 hour\r\n\r\n/**\r\n * Generate perspectives with caching\r\n */\r\nexport async function generatePerspectivesCached(\r\n  question: string,\r\n  specialistResponse: string,\r\n  context: AppContext\r\n): Promise<{\r\n  synthesized: string;\r\n  perspectives: {\r\n    optimist?: string;\r\n    critic?: string;\r\n    neutral?: string;\r\n  };\r\n}> {\r\n  const cacheKey = `${question}:${specialistResponse.substring(0, 100)}`;\r\n  const cached = perspectiveCache.get(cacheKey);\r\n\r\n  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {\r\n    return cached.perspectives as any;\r\n  }\r\n\r\n  const result = await generateMultiPerspectiveResponse(question, specialistResponse, context);\r\n\r\n  perspectiveCache.set(cacheKey, {\r\n    perspectives: result as any,\r\n    timestamp: Date.now(),\r\n  });\r\n\r\n  return result;\r\n}\r\n\r\n/**\r\n * Clear perspective cache\r\n */\r\nexport function clearPerspectiveCache(): void {\r\n  perspectiveCache.clear();\r\n}\r\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACyBO,IAAM,oBAAiD;AAAA;AAAA,EAG5D,+BAA+B;AAAA,IAC7B,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,oCAAoC;AAAA,IAClC,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,iBAAiB;AAAA,IACf,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,+BAA+B;AAAA,IAC7B,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,yBAAyB;AAAA,IACvB,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,wCAAwC;AAAA,IACtC,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA;AAAA,EAIA,sBAAsB;AAAA,IACpB,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,0BAA0B;AAAA,IACxB,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,kCAAkC;AAAA,IAChC,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,wBAAwB;AAAA,IACtB,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAQA,+BAA+B;AAAA,IAC7B,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,iBAAiB;AAAA,IACf,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA,EAEA,+BAA+B;AAAA,IAC7B,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AAAA;AAAA,EAIA,wBAAwB;AAAA,IACtB,IAAI;AAAA,IACJ,UAAU;AAAA,IACV,OAAO;AAAA,IACP,sBAAsB;AAAA,IACtB,cAAc;AAAA,MACZ,QAAQ;AAAA,MACR,WAAW;AAAA,MACX,UAAU;AAAA,IACZ;AAAA,EACF;AACF;AAKO,IAAM,gBAAwC;AAAA;AAAA,EAEnD,WAAW;AAAA,EACX,QAAQ;AAAA,EACR,SAAS;AAAA;AAAA,EAGT,UAAU;AAAA,EACV,QAAQ;AAAA,EACR,aAAa;AAAA,EACb,UAAU;AAAA,EACV,YAAY;AAAA,EACZ,eAAe;AACjB;AAiCO,IAAM,mBAAN,MAAuB;AAAA,EAI5B,YAAY,QAA0B;AACpC,SAAK,SAAS;AACd,SAAK,UAAU,OAAO,WAAW;AAAA,EACnC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,KACJ,UACA,UAAuB,CAAC,GACD;AA/Q3B;AAgRI,UAAM,QAAQ,QAAQ,SAAS;AAE/B,UAAM,cAAc;AAAA,MAClB;AAAA,MACA,UAAU,SAAS,IAAI,QAAM;AAAA,QAC3B,MAAM,EAAE;AAAA,QACR,SAAS,EAAE;AAAA,MACb,EAAE;AAAA,MACF,cAAa,aAAQ,gBAAR,YAAuB;AAAA,MACpC,aAAY,aAAQ,cAAR,YAAqB;AAAA,MACjC,OAAO,QAAQ;AAAA,MACf,SAAQ,aAAQ,WAAR,YAAkB;AAAA,IAC5B;AAEA,UAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,qBAAqB;AAAA,MAC/D,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,iBAAiB,UAAU,KAAK,OAAO,MAAM;AAAA,QAC7C,gBAAgB;AAAA,QAChB,gBAAgB,OAAO,WAAW,cAAc,OAAO,SAAS,OAAO;AAAA,QACvE,WAAW;AAAA,MACb;AAAA,MACA,MAAM,KAAK,UAAU,WAAW;AAAA,MAChC,QAAQ,QAAQ,UAAU,YAAY,QAAQ,QAAQ,OAAO,IAAI;AAAA,IACnE,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,YAAY,MAAM,SAAS,KAAK;AACtC,YAAM,IAAI;AAAA,QACR,yBAAyB,SAAS,MAAM,IAAI,SAAS,UAAU;AAAA,QAC/D,SAAS;AAAA,QACT;AAAA,MACF;AAAA,IACF;AAEA,UAAM,OAAO,MAAM,SAAS,KAAK;AAGjC,UAAM,UAAS,UAAK,YAAL,mBAAe;AAC9B,QAAI,CAAC,QAAQ;AACX,YAAM,IAAI,gBAAgB,uCAAuC,GAAG;AAAA,IACtE;AAEA,WAAO;AAAA,MACL,WAAS,YAAO,YAAP,mBAAgB,YAAW;AAAA,MACpC,OAAO,KAAK,SAAS;AAAA,MACrB,OAAO;AAAA,QACL,gBAAc,UAAK,UAAL,mBAAY,kBAAiB;AAAA,QAC3C,oBAAkB,UAAK,UAAL,mBAAY,sBAAqB;AAAA,QACnD,eAAa,UAAK,UAAL,mBAAY,iBAAgB;AAAA,MAC3C;AAAA,MACA,cAAc,OAAO;AAAA,IACvB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,eACJ,cACA,aACA,UAAuB,CAAC,GACD;AACvB,WAAO,KAAK;AAAA,MACV;AAAA,QACE,EAAE,MAAM,UAAU,SAAS,aAAa;AAAA,QACxC,EAAE,MAAM,QAAQ,SAAS,YAAY;AAAA,MACvC;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKO,WACL,IAEuC;AAAA,wDAFvC,UACA,UAAuB,CAAC,GACe;AA9V3C;AA+VI,YAAM,QAAQ,QAAQ,SAAS;AAE/B,YAAM,cAAc;AAAA,QAClB;AAAA,QACA,UAAU,SAAS,IAAI,QAAM;AAAA,UAC3B,MAAM,EAAE;AAAA,UACR,SAAS,EAAE;AAAA,QACb,EAAE;AAAA,QACF,cAAa,aAAQ,gBAAR,YAAuB;AAAA,QACpC,aAAY,aAAQ,cAAR,YAAqB;AAAA,QACjC,QAAQ;AAAA,MACV;AAEA,YAAM,WAAW,kBAAM,MAAM,GAAG,KAAK,OAAO,qBAAqB;AAAA,QAC/D,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,iBAAiB,UAAU,KAAK,OAAO,MAAM;AAAA,UAC7C,gBAAgB;AAAA,UAChB,gBAAgB;AAAA,UAChB,WAAW;AAAA,QACb;AAAA,QACA,MAAM,KAAK,UAAU,WAAW;AAAA,MAClC,CAAC;AAED,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,IAAI;AAAA,UACR,yBAAyB,SAAS,MAAM,IAAI,SAAS,UAAU;AAAA,UAC/D,SAAS;AAAA,QACX;AAAA,MACF;AAGA,YAAM,UAAS,cAAS,SAAT,mBAAe;AAC9B,UAAI,CAAC,OAAQ,OAAM,IAAI,gBAAgB,oBAAoB,GAAG;AAE9D,YAAM,UAAU,IAAI,YAAY;AAEhC,UAAI;AACF,eAAO,MAAM;AACX,gBAAM,EAAE,MAAM,MAAM,IAAI,kBAAM,OAAO,KAAK;AAC1C,cAAI,KAAM;AAEV,gBAAM,QAAQ,QAAQ,OAAO,KAAK;AAClC,gBAAM,QAAQ,MAAM,MAAM,IAAI,EAAE,OAAO,UAAQ,KAAK,KAAK,CAAC;AAE1D,qBAAW,QAAQ,OAAO;AACxB,gBAAI,KAAK,WAAW,QAAQ,GAAG;AAC7B,oBAAM,OAAO,KAAK,MAAM,CAAC;AACzB,kBAAI,SAAS,SAAU;AAEvB,kBAAI;AACF,sBAAM,SAAS,KAAK,MAAM,IAAI;AAC9B,sBAAM,WAAU,wBAAO,YAAP,mBAAiB,OAAjB,mBAAqB,UAArB,mBAA4B;AAC5C,oBAAI,QAAS,OAAM;AAAA,cACrB,SAAQ;AAAA,cAER;AAAA,YACF;AAAA,UACF;AAAA,QACF;AAAA,MACF,UAAE;AACA,eAAO,YAAY;AAAA,MACrB;AAAA,IACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,SAAS,gBAAiD;AAExD,UAAM,UAAU,cAAc,cAAc,KAAK;AACjD,WAAO,kBAAkB,OAAO;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA,EAKA,eAA4C;AAC1C,WAAO,mBAAK;AAAA,EACd;AACF;AAMO,IAAM,kBAAN,cAA8B,MAAM;AAAA,EACzC,YACE,SACO,YACA,cACP;AACA,UAAM,OAAO;AAHN;AACA;AAGP,SAAK,OAAO;AAAA,EACd;AACF;AASO,SAAS,yBAA2C;AACzD,QAAM,SAAS,QAAQ,IAAI,sBAAsB;AAEjD,MAAI,CAAC,QAAQ;AACX,UAAM,IAAI,MAAM,oDAAoD;AAAA,EACtE;AAEA,SAAO,IAAI,iBAAiB;AAAA,IAC1B;AAAA,IACA,SAAS;AAAA;AAAA,EACX,CAAC;AACH;;;AC1bA,IAAM,iBAA6C;AAAA;AAAA,EAEjD,yBAAyB,CAAC,sBAAsB,+BAA+B,wBAAwB;AAAA,EACvG,oBAAoB,CAAC,sBAAsB,6BAA6B;AAAA;AAAA,EAGxE,uBAAuB,CAAC,+BAA+B,0BAA0B,eAAe;AAAA,EAChG,0BAA0B,CAAC,0BAA0B,sBAAsB,6BAA6B;AAAA,EACxG,aAAa,CAAC,+BAA+B,wBAAwB;AAAA;AAAA,EAGrE,eAAe,CAAC,+BAA+B,iBAAiB,uBAAuB;AAAA;AAAA,EAGvF,mBAAmB,CAAC,sBAAsB,6BAA6B;AACzE;AAKA,IAAM,yBAAmD;AAAA,EACvD,QAAQ,CAAC,sBAAsB,+BAA+B,wBAAwB;AAAA,EACtF,WAAW,CAAC,+BAA+B,0BAA0B,iBAAiB,6BAA6B;AAAA,EACnH,UAAU,CAAC,+BAA+B,wBAAwB,eAAe;AACnF;AASO,IAAM,cAAN,MAAkB;AAAA,EAGvB,cAAc;AACZ,SAAK,SAAS,uBAAuB;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA,EAKA,YAAY,MAAgB,SAAkC;AAC5D,UAAM,kBAAkB,eAAe,IAAI,KAAK,eAAe,qBAAqB;AACpF,UAAM,EAAE,MAAM,iBAAiB,IAAI,QAAQ;AAG3C,UAAM,aAAa,uBAAuB,IAAI,KAAK;AACnD,UAAM,iBAAiB,gBAAgB,OAAO,OAAK,WAAW,SAAS,CAAC,CAAC;AAGzE,QAAI,mBAAmB,KAAM;AAC3B,aAAO,KAAK,iBAAiB,cAAc;AAAA,IAC7C;AAGA,UAAM,UAAU,eAAe,CAAC;AAChC,UAAM,QAAQ,kBAAkB,OAAO;AAEvC,QAAI,CAAC,OAAO;AACV,YAAM,IAAI,MAAM,oBAAoB,OAAO,EAAE;AAAA,IAC/C;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,KACJ,cACA,aACA,UAAuB,CAAC,GACD;AACvB,WAAO,KAAK,OAAO,eAAe,cAAc,aAAa,OAAO;AAAA,EACtE;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aACJ,UACA,UAAuB,CAAC,GACD;AACvB,WAAO,KAAK,OAAO,KAAK,UAAU,OAAO;AAAA,EAC3C;AAAA;AAAA;AAAA;AAAA,EAKO,WACL,IAEwB;AAAA,wDAFxB,UACA,UAAuB,CAAC,GACA;AACxB,yBAAO,KAAK,OAAO,WAAW,UAAU,OAAO;AAAA,IACjD;AAAA;AAAA;AAAA;AAAA;AAAA,EAKQ,iBAAiB,UAAiC;AACxD,QAAI,WAAW,kBAAkB,SAAS,CAAC,CAAC;AAC5C,QAAI,cAAa,qCAAU,yBAAwB;AAEnD,eAAW,WAAW,UAAU;AAC9B,YAAM,QAAQ,kBAAkB,OAAO;AACvC,UAAI,SAAS,MAAM,uBAAuB,YAAY;AACpD,mBAAW;AACX,qBAAa,MAAM;AAAA,MACrB;AAAA,IACF;AAEA,WAAO,YAAY,kBAAkB,oBAAoB;AAAA,EAC3D;AAAA;AAAA;AAAA;AAAA,EAKA,SAAS,WAA4C;AACnD,UAAM,UAAU,cAAc,SAAS,KAAK;AAC5C,WAAO,kBAAkB,OAAO;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA,EAKA,eAA4C;AAC1C,WAAO,mBAAK;AAAA,EACd;AACF;AAGA,IAAI,iBAAqC;AAKlC,SAAS,iBAA8B;AAC5C,MAAI,CAAC,gBAAgB;AACnB,qBAAiB,IAAI,YAAY;AAAA,EACnC;AACA,SAAO;AACT;;;ACxJO,IAAM,kBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAgBxB,IAAM,gBAAgB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAgBtB,IAAM,iBAAiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAevB,IAAM,mBAAmB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAwBhC,eAAsB,iCACpB,UACA,oBACA,SAQC;AAED,QAAM,qBAAqB;AAC3B,QAAM,mBAAmB;AAEzB,MAAI;AAEF,UAAM,CAAC,UAAU,QAAQ,OAAO,IAAI,MAAM,QAAQ,IAAI;AAAA,MACpD,oBAAoB,UAAU,oBAAoB,YAAY,SAAS,kBAAkB;AAAA,MACzF,oBAAoB,UAAU,oBAAoB,UAAU,SAAS,kBAAkB;AAAA,MACvF,oBAAoB,UAAU,oBAAoB,WAAW,SAAS,kBAAkB;AAAA,IAC1F,CAAC;AAGD,UAAM,cAAc,MAAM,mBAAmB,UAAU,oBAAoB;AAAA,MACzE;AAAA,MACA;AAAA,MACA;AAAA,IACF,GAAG,SAAS,gBAAgB;AAE5B,WAAO;AAAA,MACL;AAAA,MACA,cAAc;AAAA,QACZ;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,EACF,SAAS,OAAO;AAEd,YAAQ,MAAM,kCAAkC,KAAK;AACrD,WAAO;AAAA,MACL,aAAa,GAAG,kBAAkB;AAAA;AAAA;AAAA,MAClC,cAAc;AAAA,IAChB;AAAA,EACF;AACF;AAKA,eAAe,oBACb,UACA,oBACA,MACA,SACA,SACiB;AACjB,QAAM,SAAS,eAAe;AAC9B,QAAM,SAAS,qBAAqB,IAAI;AAExC,QAAM,cAAc;AAAA,gBACN,QAAQ;AAAA;AAAA,2BAEG,kBAAkB;AAAA;AAAA,eAE9B,IAAI;AAEjB,MAAI;AACF,UAAM,WAAW,MAAM,OAAO;AAAA,MAC5B;AAAA,MACA;AAAA,MACA;AAAA,QACE,OAAO;AAAA,QACP,aAAa;AAAA,QACb,WAAW;AAAA;AAAA,MACb;AAAA,IACF;AAEA,WAAO,SAAS,QAAQ,KAAK;AAAA,EAC/B,SAAS,OAAO;AAEd,WAAO,uBAAuB,MAAM,QAAQ;AAAA,EAC9C;AACF;AAKA,SAAS,qBAAqB,MAA+B;AAC3D,UAAQ,MAAM;AAAA,IACZ,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,EACX;AACF;AAKA,eAAe,mBACb,UACA,oBACA,cAKA,SACA,SACiB;AACjB,QAAM,SAAS,eAAe;AAE9B,QAAM,cAAc;AAAA,gBACN,QAAQ;AAAA;AAAA;AAAA,EAGtB,kBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA,EAKlB,aAAa,QAAQ;AAAA;AAAA;AAAA,EAGrB,aAAa,MAAM;AAAA;AAAA;AAAA,EAGnB,aAAa,OAAO;AAAA;AAAA;AAIpB,MAAI;AACF,UAAM,WAAW,MAAM,OAAO;AAAA,MAC5B;AAAA,MACA;AAAA,MACA;AAAA,QACE,OAAO;AAAA,QACP,aAAa;AAAA,QACb,WAAW;AAAA,MACb;AAAA,IACF;AAEA,WAAO,SAAS,QAAQ,KAAK;AAAA,EAC/B,SAAS,OAAO;AAEd,WAAO,wBAAwB,oBAAoB,YAAY;AAAA,EACjE;AACF;AAKA,SAAS,uBAAuB,MAAuB,UAA0B;AAC/E,UAAQ,MAAM;AAAA,IACZ,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,EACX;AACF;AAKA,SAAS,wBACP,oBACA,cAKQ;AACR,SAAO,GAAG,kBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAO5B,aAAa,QAAQ;AAAA;AAAA;AAAA,EAGrB,aAAa,MAAM;AAAA;AAAA;AAAA,EAGnB,aAAa,OAAO;AACtB;AASA,IAAM,mBAAmB,oBAAI,IAG1B;AAEH,IAAM,YAAY,MAAO,KAAK;AAK9B,eAAsB,2BACpB,UACA,oBACA,SAQC;AACD,QAAM,WAAW,GAAG,QAAQ,IAAI,mBAAmB,UAAU,GAAG,GAAG,CAAC;AACpE,QAAM,SAAS,iBAAiB,IAAI,QAAQ;AAE5C,MAAI,UAAU,KAAK,IAAI,IAAI,OAAO,YAAY,WAAW;AACvD,WAAO,OAAO;AAAA,EAChB;AAEA,QAAM,SAAS,MAAM,iCAAiC,UAAU,oBAAoB,OAAO;AAE3F,mBAAiB,IAAI,UAAU;AAAA,IAC7B,cAAc;AAAA,IACd,WAAW,KAAK,IAAI;AAAA,EACtB,CAAC;AAED,SAAO;AACT;AAKO,SAAS,wBAA8B;AAC5C,mBAAiB,MAAM;AACzB;","names":[]}